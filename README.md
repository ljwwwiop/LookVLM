# 🌟 LooKVLM

**LooKVLM**: *Lightweight Omni Open Knowledge Vision-Language Model*

> Say goodbye to bulky models! LooKVLM is a compact yet capable VLM designed for efficiency, accessibility, and open knowledge understanding.

---

## 🧠 What is LooKVLM?

**LooKVLM** is a lightweight, open-source **vision-language model (VLM)** that supports **multi-modal input**, **external knowledge grounding**, and **efficient inference** on limited hardware. Built for versatility and research accessibility, it bridges vision and language understanding under a unified, streamlined framework.

---

## ✨ Key Features

- ⚡ **Lightweight**: Runs efficiently on a single GPU (RTX 3090); ideal for academic, mobile, or edge scenarios.
- 🔎 **Omni-modal Input**: Supports text, images, documents, diagrams, and structured context.
- 🌐 **Open Knowledge Fusion**: Integrates external knowledge into multi-modal reasoning pipelines.
- 🔧 **Modular and Extensible**: Easy to plug in new adapters, prompts, or training heads.
- 💬 **LLM-Ready**: Designed for LLaVA-style multi-turn visual conversation and grounding.

---

## 🚀 Quick Start
