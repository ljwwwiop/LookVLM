# ğŸŒŸ LooKVLM

**LooKVLM**: *Lightweight Omni Open Knowledge Vision-Language Model*

> Say goodbye to bulky models! LooKVLM is a compact yet capable VLM designed for efficiency, accessibility, and open knowledge understanding.

---

## ğŸ§  What is LooKVLM?

**LooKVLM** is a lightweight, open-source **vision-language model (VLM)** that supports **multi-modal input**, **external knowledge grounding**, and **efficient inference** on limited hardware. Built for versatility and research accessibility, it bridges vision and language understanding under a unified, streamlined framework.

---

## âœ¨ Key Features

- âš¡ **Lightweight**: Runs efficiently on a single GPU (RTX 3090); ideal for academic, mobile, or edge scenarios.
- ğŸ” **Omni-modal Input**: Supports text, images, documents, diagrams, and structured context.
- ğŸŒ **Open Knowledge Fusion**: Integrates external knowledge into multi-modal reasoning pipelines.
- ğŸ”§ **Modular and Extensible**: Easy to plug in new adapters, prompts, or training heads.
- ğŸ’¬ **LLM-Ready**: Designed for LLaVA-style multi-turn visual conversation and grounding.

---

## ğŸš€ Quick Start
